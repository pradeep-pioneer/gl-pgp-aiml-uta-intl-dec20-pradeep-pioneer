{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We cannot work with the text data in machine learning so we need to convert them into numerical vectors, As a part of this practice exercise you will implement different techniques to do the same.\n",
    "\n",
    "### In this notebook we are going to understand techniques for encoding text data. We are going to learn about\n",
    "\n",
    "1. **Techniques for Encoding** - These are the popular techniques that are used for encoding:\n",
    "    * **Bag of Words**\n",
    "    * **TF-IDF**( **T**erm **F**requency - **I**nverse **D**ocument **F**requency)\n",
    "2. **Sentiment Analysis** - Sentiment Analysis is the process of ‘computationally’ determining whether a piece of writing is positive, negative or neutral. The below can be used for Sentiment Analysis:\n",
    "    * **TextBlob**\n",
    "    * **VADER Sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PradeepSingh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PradeepSingh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np                                  #for large and multi-dimensional arrays\n",
    "import pandas as pd                                 #for data manipulation and analysis\n",
    "import nltk                                         #Natural language processing tool-kit\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords                   #Stopwords corpus\n",
    "from nltk.stem import PorterStemmer                 # Stemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 'I enjoy this program.'\n",
    "d2 = 'This program is great.'\n",
    "d3 = 'This product is not great.'\n",
    "d4 = 'I really love this brand.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-processing Steps:\n",
    "\n",
    "* Conversion to lowercase.\n",
    "* Removal of punctuation.\n",
    "* Tokenization.\n",
    "* Stopwords removal except the word 'not'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def preprocess(text):\n",
    "  '''Pre-processing steps as described above.'''\n",
    "  # text = text.lower()\n",
    "  # # text = re.sub(r'[.|,|)|(|\\|/]',r'', text)        #Removing Punctuations\n",
    "  # words = [word for word in text if word not in stopwords]   # removing stopwords\n",
    "  # return words\n",
    "  text = text.lower()                 # Converting to lowercase\n",
    "  text = re.sub(r'[.|,|)|(|\\|/]',r' ', text) # Removing punctuation\n",
    "  word_tokens = word_tokenize(text)  # Tokenization\n",
    "\n",
    "  words = [word for word in word_tokens if word not in stopwords] # Stop word removal\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'program', 'program', 'great', 'product', 'not', 'great', 'really', 'love', 'brand']\n"
     ]
    }
   ],
   "source": [
    "d1_new = preprocess(d1)\n",
    "d2_new = preprocess(d2)\n",
    "d3_new = preprocess(d3)\n",
    "d4_new = preprocess(d4)\n",
    "\n",
    "sent = d1_new + d2_new + d3_new + d4_new\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS\n",
    "In BoW we construct a dictionary that contains set of all unique words from our text review dataset. The frequency of the word is counted here. If there are d unique words in our dictionary then for every sentence or review the vector will be of length d and count of word from review is stored at its particular location in vector. The vector will be highly sparse in such case.\n",
    "\n",
    "### Using scikit-learn's CountVectorizer we can get the BoW and check out all the parameters it consists of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enjoy': 1, 'program': 6, 'great': 2, 'product': 5, 'not': 4, 'really': 7, 'love': 3, 'brand': 0}\n",
      "(10, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()  \n",
    "X = cv.fit_transform(sent)\n",
    "print(cv.vocabulary_)\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product', 'really', 'not', 'love', 'brand', 'program', 'enjoy', 'great']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvocabulary = list(set(sent))\n",
    "myvocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "**Term Frequency - Inverse Document Frequency** it makes sure that less importance is given to most frequent words and also considers less frequent words.\n",
    "\n",
    "**Term Frequency** is number of times a particular word(W) occurs in a review divided by totall number of words (Wr) in review. The term frequency value ranges from 0 to 1.\n",
    "\n",
    "**Inverse Document Frequency** is calculated as **log(Total Number of Docs(N) / Number of Docs which contains particular word(n))**. Here Docs referred as Reviews.\n",
    "\n",
    "**TF-IDF** is **TF * IDF** that is **(W/Wr)*LOG(N/n)**\n",
    "\n",
    "Using scikit-learn's tfidfVectorizer we can get the TF-IDF.\n",
    "\n",
    "So even here we get a TF-IDF value for every word and in some cases it may consider different meaning reviews as similar after stopwords removal. so to over come we can use BI-Gram or NGram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product': 0, 'really': 1, 'not': 2, 'love': 3, 'brand': 4, 'program': 5, 'enjoy': 6, 'great': 7}\n",
      "[1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.51082562\n",
      " 1.91629073 1.51082562]\n",
      "(4, 8)\n",
      "                1         2         3        4\n",
      "product  0.000000  0.000000  0.617614  0.00000\n",
      "really   0.000000  0.000000  0.000000  0.57735\n",
      "not      0.000000  0.000000  0.617614  0.00000\n",
      "love     0.000000  0.000000  0.000000  0.57735\n",
      "brand    0.000000  0.000000  0.000000  0.57735\n",
      "program  0.619130  0.707107  0.000000  0.00000\n",
      "enjoy    0.785288  0.000000  0.000000  0.00000\n",
      "great    0.000000  0.707107  0.486934  0.00000\n"
     ]
    }
   ],
   "source": [
    "corpus = {1: d1, 2: d2, 3: d3, 4: d4}\n",
    "tfidf = TfidfVectorizer(vocabulary = myvocabulary, ngram_range = (1,2))\n",
    "tfs = tfidf.fit_transform(corpus.values())\n",
    "feature_names = tfidf.get_feature_names()\n",
    "corpus_index = [n for n in corpus]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfs.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(tfidf.vocabulary_)\n",
    "print(tfidf.idf_)\n",
    "print(tfs.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VADER (Valence Aware Dictionary and sEntiment Reasoner)** is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative. VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy this program.: {'neg': 0.0, 'neu': 0.484, 'pos': 0.516, 'compound': 0.4939}\n",
      "This program is great.: {'neg': 0.0, 'neu': 0.423, 'pos': 0.577, 'compound': 0.6249}\n",
      "This product is not great.: {'neg': 0.452, 'neu': 0.548, 'pos': 0.0, 'compound': -0.5096}\n",
      "I really love this brand.: {'neg': 0.0, 'neu': 0.471, 'pos': 0.529, 'compound': 0.6697}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "sentiment_dict1 = sentiment.polarity_scores(d1)\n",
    "sentiment_dict2 = sentiment.polarity_scores(d2)\n",
    "sentiment_dict3 = sentiment.polarity_scores(d3)\n",
    "sentiment_dict4 = sentiment.polarity_scores(d4)\n",
    "\n",
    "print(f'{d1}: {sentiment_dict1}')\n",
    "print(f'{d2}: {sentiment_dict2}')\n",
    "print(f'{d3}: {sentiment_dict3}')\n",
    "print(f'{d4}: {sentiment_dict4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy this program.: Sentiment(polarity=0.4, subjectivity=0.5)\n",
      "This program is great.: Sentiment(polarity=0.8, subjectivity=0.75)\n",
      "This product is not great.: Sentiment(polarity=-0.4, subjectivity=0.75)\n",
      "I really love this brand.: Sentiment(polarity=0.5, subjectivity=0.6)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "print(f'{d1}: {TextBlob(d1).sentiment}')\n",
    "print(f'{d2}: {TextBlob(d2).sentiment}')\n",
    "print(f'{d3}: {TextBlob(d3).sentiment}')\n",
    "print(f'{d4}: {TextBlob(d4).sentiment}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GL-Python-3.7 (tensorflow-gpu)",
   "language": "python",
   "name": "gl-tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
