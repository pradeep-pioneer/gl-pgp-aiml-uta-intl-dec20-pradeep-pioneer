{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to go from what we will describe as a chunk of text (not to be confused with text chunking), a lengthy, unprocessed single string, and end up with a list (or several lists) of cleaned tokens that would be useful for further text mining and/or natural language processing tasks.\n",
    "\n",
    "* NLTK - The Natural Language ToolKit is one of the best-known and most-used NLP libraries in the Python ecosystem, useful for all sorts of tasks from tokenization, to stemming, and beyond\n",
    "\n",
    "* BeautifulSoup - BeautifulSoup is a useful library for extracting data from HTML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import re, string, unicodedata\n",
    "import nltk                                   # Natural language processing tool-kit\n",
    "import contractions\n",
    "\n",
    "from bs4 import BeautifulSoup                 # Beautiful soup is a parsing library that can use different parsers.\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet    # Stopwords, and wordnet corpus\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<h1>This is the title</h1>\n",
    "            <b>This is bold text</b>\n",
    "            <i>This is italicized Text</i>\n",
    "            <img src=\"another html tag\"/>\n",
    "            <a href=\"Apart from the others\"> This is also here!</a>\n",
    "            “Love all, trust a few, do wrong to none.” \n",
    "            ― William Shakespeare, All's Well That Ends Well\n",
    "\n",
    "            “All the world's a stage,\n",
    "            And all the men and women merely players;\n",
    "            They have their exits and their entrances;\n",
    "            And one man in his time plays many parts,\n",
    "            His acts being seven ages.” \n",
    "            ― William Shakespeare, As You Like It\n",
    "\n",
    "            \"How old are you,\" asked Jem, \"four-and-a-half?\"\n",
    "\n",
    "            \"Goin' on seven.\"\n",
    "\n",
    "            \"Shoot no wonder, then,\" said Jem, jerking his thumb at me. \"Scout yonder's been readin' ever since she was born, \n",
    "            and she ain't even started to school yet. You look right puny for goin' on seven.\"\n",
    "\n",
    "            \"I'm little but I'm old,\" he said.\n",
    "            - To Kill a Mockingbird\n",
    "\n",
    "            Le dîner, Clémence, Anaïs, Raphaël, Voilà !\n",
    "\n",
    "            something... is! not right() with.,; this :: line.\n",
    "            \n",
    "            &nbsp;&nbsp;\n",
    "            \n",
    "            11    42   1024   2048\n",
    "            {{There are double curly braces.}}\n",
    "            {Here are single curly braces.}\n",
    "            </body>\n",
    "            </html>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Removal\n",
    "Let's define noise removal as text-specific normalization tasks which often take place prior to tokenization.\n",
    "\n",
    "* While the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific.\n",
    "\n",
    "Noise removal tasks could include:\n",
    "\n",
    "* Removing text file headers, footers\n",
    "* Removing HTML, XML, etc. markup and metadata\n",
    "* Extracting valuable data from other formats, such as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the title\n",
      "This is bold text\n",
      "This is italicized Text\n",
      "\n",
      " This is also here!\n",
      "            “Love all, trust a few, do wrong to none.” \n",
      "            ― William Shakespeare, All's Well That Ends Well\n",
      "\n",
      "            “All the world's a stage,\n",
      "            And all the men and women merely players;\n",
      "            They have their exits and their entrances;\n",
      "            And one man in his time plays many parts,\n",
      "            His acts being seven ages.” \n",
      "            ― William Shakespeare, As You Like It\n",
      "\n",
      "            \"How old are you,\" asked Jem, \"four-and-a-half?\"\n",
      "\n",
      "            \"Goin' on seven.\"\n",
      "\n",
      "            \"Shoot no wonder, then,\" said Jem, jerking his thumb at me. \"Scout yonder's been readin' ever since she was born, \n",
      "            and she ain't even started to school yet. You look right puny for goin' on seven.\"\n",
      "\n",
      "            \"I'm little but I'm old,\" he said.\n",
      "            - To Kill a Mockingbird\n",
      "\n",
      "            Le dîner, Clémence, Anaïs, Raphaël, Voilà !\n",
      "\n",
      "            something... is! not right() with.,; this :: line.\n",
      "            \n",
      "              \n",
      "            \n",
      "            11    42   1024   2048\n",
      "            {{There are double curly braces.}}\n",
      "            {Here are single curly braces.}\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the code to remove all the html tags from the text string. And print the processed text.\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")    # Removing HTML tags\n",
    "    return soup.get_text()\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    # Any other step can also be added here according to need e.g we can add code to remove string inside the curly braces.\n",
    "    return text\n",
    "text = denoise_text(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not mandatory to do at this stage prior to tokenization but:\n",
    "\n",
    "Replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\"\n",
    "It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the title\n",
      "This is bold text\n",
      "This is italicized Text\n",
      "\n",
      " This is also here!\n",
      "            “Love all, trust a few, do wrong to none.” \n",
      "            ― William Shakespeare, All's Well That Ends Well\n",
      "\n",
      "            “All the world's a stage,\n",
      "            And all the men and women merely players;\n",
      "            They have their exits and their entrances;\n",
      "            And one man in his time plays many parts,\n",
      "            His acts being seven ages.” \n",
      "            ― William Shakespeare, As You Like It\n",
      "\n",
      "            \"How old are you,\" asked Jem, \"four-and-a-half?\"\n",
      "\n",
      "            \"going on seven.\"\n",
      "\n",
      "            \"Shoot no wonder, then,\" said Jem, jerking his thumb at me. \"Scout yonder's been readin' ever since she was born, \n",
      "            and she are not even started to school yet. You look right puny for going on seven.\"\n",
      "\n",
      "            \"I am little but I am old,\" he said.\n",
      "            - To Kill a Mockingbird\n",
      "\n",
      "            Le dîner, Clémence, Anaïs, Raphaël, Voilà !\n",
      "\n",
      "            something... is! not right() with.,; this :: line.\n",
      "            \n",
      "              \n",
      "            \n",
      "            11    42   1024   2048\n",
      "            {{There are double curly braces.}}\n",
      "            {Here are single curly braces.}\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "text = replace_contractions(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "* Tokenization is a step which splits longer strings of text into smaller pieces, or tokens.\n",
    "* Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc.\n",
    "* Further processing is generally performed after a piece of text has been appropriately tokenized.\n",
    "* Tokenization is also referred to as text segmentation or lexical analysis.\n",
    "* Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For our task, we will tokenize our sample text into a list of words. This is done using NTLK's word_tokenize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PradeepSingh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'title', 'This', 'is', 'bold', 'text', 'This', 'is', 'italicized', 'Text', 'This', 'is', 'also', 'here', '!', '“', 'Love', 'all', ',', 'trust', 'a', 'few', ',', 'do', 'wrong', 'to', 'none.', '”', '―', 'William', 'Shakespeare', ',', 'All', \"'s\", 'Well', 'That', 'Ends', 'Well', '“', 'All', 'the', 'world', \"'s\", 'a', 'stage', ',', 'And', 'all', 'the', 'men', 'and', 'women', 'merely', 'players', ';', 'They', 'have', 'their', 'exits', 'and', 'their', 'entrances', ';', 'And', 'one', 'man', 'in', 'his', 'time', 'plays', 'many', 'parts', ',', 'His', 'acts', 'being', 'seven', 'ages.', '”', '―', 'William', 'Shakespeare', ',', 'As', 'You', 'Like', 'It', '``', 'How', 'old', 'are', 'you', ',', \"''\", 'asked', 'Jem', ',', '``', 'four-and-a-half', '?', \"''\", '``', 'going', 'on', 'seven', '.', \"''\", '``', 'Shoot', 'no', 'wonder', ',', 'then', ',', \"''\", 'said', 'Jem', ',', 'jerking', 'his', 'thumb', 'at', 'me', '.', '``', 'Scout', 'yonder', \"'s\", 'been', 'readin', \"'\", 'ever', 'since', 'she', 'was', 'born', ',', 'and', 'she', 'are', 'not', 'even', 'started', 'to', 'school', 'yet', '.', 'You', 'look', 'right', 'puny', 'for', 'going', 'on', 'seven', '.', \"''\", '``', 'I', 'am', 'little', 'but', 'I', 'am', 'old', ',', \"''\", 'he', 'said', '.', '-', 'To', 'Kill', 'a', 'Mockingbird', 'Le', 'dîner', ',', 'Clémence', ',', 'Anaïs', ',', 'Raphaël', ',', 'Voilà', '!', 'something', '...', 'is', '!', 'not', 'right', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'line', '.', '11', '42', '1024', '2048', '{', '{', 'There', 'are', 'double', 'curly', 'braces', '.', '}', '}', '{', 'Here', 'are', 'single', 'curly', 'braces', '.', '}']\n",
      "Number of words is:  226\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "words = nltk.word_tokenize(text)     # list of words.\n",
    "print(words)\n",
    "print('Number of words is: ', len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "* converting all text to the same case (upper or lower), removing punctuation, and so on.\n",
    "* Steps:\n",
    "\n",
    "    * Removal of non-ASCII characters.\n",
    "    * Conversion of all characters to lowercase.\n",
    "    * Removal of Punctuation.\n",
    "    * Stop word removal.\n",
    "    * Stemming / Lemmatization\n",
    "    * After tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this. Function names and comments should provide the necessary insight into what each does.\n",
    "\n",
    "Converting all words to lowercase and removing punctuations.\n",
    "\n",
    "**Stemming:** Converting the words into their base word or stem word ( Ex - tastefully, tasty, these words are converted to stem word called 'tasti'). This reduces the vector dimension because we dont consider all similar words\n",
    "\n",
    "**Stopwords:** Stopwords are the unnecessary words that even if they are removed the sentiment of the sentence dosent change.\n",
    "\n",
    "Ex - **This pasta is so tasty ==> pasta tasty** ( This , is, so are stopwords so they are removed)\n",
    "\n",
    "Hint:\n",
    "\n",
    "* Use regular expressions to remove punctuations.\n",
    "\n",
    "To see all the steps, run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PradeepSingh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'bold', 'text', 'italicized', 'text', 'also', 'love', 'trust', 'wrong', 'none', 'william', 'shakespeare', 'well', 'ends', 'well', 'world', 'stage', 'men', 'women', 'merely', 'players', 'exits', 'entrances', 'one', 'man', 'time', 'plays', 'many', 'parts', 'acts', 'seven', 'ages', 'william', 'shakespeare', 'like', 'old', 'asked', 'jem', 'fourandahalf', 'going', 'seven', 'shoot', 'wonder', 'said', 'jem', 'jerking', 'thumb', 'scout', 'yonder', 'readin', 'ever', 'since', 'born', 'even', 'started', 'school', 'yet', 'look', 'right', 'puny', 'going', 'seven', 'little', 'old', 'said', 'kill', 'mockingbird', 'le', 'diner', 'clemence', 'anais', 'raphael', 'voila', 'something', 'right', 'line', '11', '42', '1024', '2048', 'double', 'curly', 'braces', 'single', 'curly', 'braces']\n",
      "Number of words is:  86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)        # Append processed words to new list.\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        new_word = word.lower()           # Converting to lowercase\n",
    "        new_words.append(new_word)        # Append processed words to new list.\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)    # Append processed words to new list.\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []                        # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)        # Append processed words to new list.\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []                            # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)                # Append processed words to new list.\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []                           # Create empty list to store pre-processed words.\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)              # Append processed words to new list.\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "nltk.download('stopwords')\n",
    "words = normalize(words)\n",
    "print(words)\n",
    "print('Number of words is: ', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PradeepSingh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed:\n",
      " ['titl', 'bold', 'text', 'it', 'text', 'also', 'lov', 'trust', 'wrong', 'non', 'william', 'shakespear', 'wel', 'end', 'wel', 'world', 'stag', 'men', 'wom', 'mer', 'play', 'exit', 'ent', 'on', 'man', 'tim', 'play', 'many', 'part', 'act', 'sev', 'ag', 'william', 'shakespear', 'lik', 'old', 'ask', 'jem', 'fourandahalf', 'going', 'sev', 'shoot', 'wond', 'said', 'jem', 'jerk', 'thumb', 'scout', 'yond', 'readin', 'ev', 'sint', 'born', 'ev', 'start', 'school', 'yet', 'look', 'right', 'puny', 'going', 'sev', 'littl', 'old', 'said', 'kil', 'mockingbird', 'le', 'din', 'cle', 'ana', 'raphael', 'voil', 'someth', 'right', 'lin', '11', '42', '1024', '2048', 'doubl', 'cur', 'brac', 'singl', 'cur', 'brac']\n",
      "\n",
      "Lemmatized:\n",
      " ['title', 'bold', 'text', 'italicize', 'text', 'also', 'love', 'trust', 'wrong', 'none', 'william', 'shakespeare', 'well', 'end', 'well', 'world', 'stage', 'men', 'women', 'merely', 'players', 'exit', 'entrance', 'one', 'man', 'time', 'play', 'many', 'part', 'act', 'seven', 'age', 'william', 'shakespeare', 'like', 'old', 'ask', 'jem', 'fourandahalf', 'go', 'seven', 'shoot', 'wonder', 'say', 'jem', 'jerk', 'thumb', 'scout', 'yonder', 'readin', 'ever', 'since', 'bear', 'even', 'start', 'school', 'yet', 'look', 'right', 'puny', 'go', 'seven', 'little', 'old', 'say', 'kill', 'mockingbird', 'le', 'diner', 'clemence', 'anais', 'raphael', 'voila', 'something', 'right', 'line', '11', '42', '1024', '2048', 'double', 'curly', 'brace', 'single', 'curly', 'brace']\n"
     ]
    }
   ],
   "source": [
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "nltk.download('wordnet')\n",
    "stems, lemmas = stem_and_lemmatize(words)\n",
    "print('Stemmed:\\n', stems)\n",
    "print('\\nLemmatized:\\n', lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a return of 2 new lists: \n",
    "* one of stemmed tokens\n",
    "* and another of lemmatized tokens with respect to verbs.\n",
    "Depending on your upcoming NLP task or preference, one of these may be more appropriate than the other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GL-Python-3.7 (tensorflow-gpu)",
   "language": "python",
   "name": "gl-tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
